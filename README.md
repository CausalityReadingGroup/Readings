# Resources on causal inference


## Discussions

### Judea Pearl

#### A Statistician’s Re-Reaction to The Book of Why

link: http://causality.cs.ucla.edu/blog/index.php/2018/06/15/a-statisticians-re-reaction-to-the-book-of-why/

> It took quite a risk on my part to sound pretentious and call this development a Causal Revolution. I thought it was necessary. Now I am asking you to take a few minutes and judge for yourself whether the evidence does not justify such a risky characterization.

> It would be nice if we could alert practicing statisticians, deeply invested in the language of statistics to the possibility that paradigm shifts can occur even in the 21st century, and that centuries of unproductive debates do not make such shifts impossible.


## Papers

### Judea Pearl


#### Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution

link: https://arxiv.org/pdf/1801.04016.pdf

> The philosopher Stephen Toulmin (1961) identifies model-based vs. model-blind dichotomy as the key
to understanding the ancient rivalry between Babylonian and Greek science. According to Toulmin, the Babylonians astronomers were masters of black-box prediction, far surpassing their Greek rivals in accuracy and consistency (Toulmin, 1961, pp. 27–30). Yet Science favored the creative-speculative strategy of the Greek astronomers which was wild with metaphysical imagery: circular tubes full of fire, small holes through which celestial fire was visible as stars, and hemispherical earth riding on turtle backs. Yet it was this wild modeling strategy, not Babylonian rigidity, that jolted Eratosthenes (276-194 BC) to perform one of the most creative experiments in the ancient world and measure the radius of the earth. This would never have occurred to a Babylonian curve-fitter.
> Coming back to strong AI, we have seen that model-blind approaches have intrinsic limitations on the cognitive tasks that they can perform. We have described some of these tasks and demonstrated how they can be accomplished in the SCM framework, and why a model-based approach is essential for performing these tasks. Our general conclusion is that human-level AI cannot emerge solely from model-blind learning machines; it requires the symbiotic collaboration of data and models.
Data science is only as much of a science as it facilitates the interpretation of data – a two-body problem, connecting data to reality. Data alone are hardly a science, regardless how big they get and how skillfully they are manipulated.

### Peter Bühlmann

#### Statistics for big data: A perspective

link: https://www.sciencedirect.com/science/article/pii/S0167715218300610?via%3Dihub

> We are surrounded by very smart computers. The computer has learned, maybe in a supervised setting, maybe unsupervised or reinforced. But we, what have we learned?

> Today’s algorithms are complex and often hardly understood. Many are not understood by most of us, and some are not understood by any of us.

> The modern adaptive algorithms are often such that the better they are (generate outcomes close to the target), the harder it is to estimate their uncertainty. For the ‘‘best’’ algorithm it can be simply impossible to know its accuracy.

>The scientific motivation for analyzing data is to increase knowledge. Think for example at finding out which gene is associated with a particular phenotype. A classical statistical uncertainty measure is the p-value. If it is small one has found something. Then the result is called significant and published in scientific journals. Lately, some panic is going on as it turned out that many findings are not replicable. The statistical uncertainty measure seems not to be reliable! The p-value is cheating! However, there is no reason for surprise as there is no outlet for publishing insignificant results (at least in domains where statistical significance is required). Maybe editors should invite authors to write about their journey along the path of insignificant results that led to the significant one. There is moreover also no reason for panic. False positives are part of the process of gaining knowledge and insight. The significant findings are to subjected to further studies and sense has to be made out of it. Thus statistics plays its role in understanding algorithms, the value of their outcomes and consequent decisions for new research directions.

## Books

### Jonas Peters

#### Elements of Causal Inference

link: https://mitpress.mit.edu/books/elements-causal-inference

Page 44
> In other words, is the structure identifiable from the joint distribution? The following known result shows that the answer is “no” if one allows for general SCMs.
